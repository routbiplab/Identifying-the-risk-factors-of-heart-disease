{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086527a1",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3de340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ebcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for any duplicate values\n",
    "df_duplicate=df[df.duplicated()].index\n",
    "df_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicates if any\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63311507",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the correlation between the variables\n",
    "df.corr()\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eb35d",
   "metadata": {},
   "source": [
    "# 2. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## selecting the features that most important to the outcome\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "x=df.iloc[:,0:13]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "\n",
    "## apply selectKbest to get top features\n",
    "top_features=SelectKBest(score_func=chi2,k=10)\n",
    "fit=top_features.fit(x,y)\n",
    "df_scores=pd.DataFrame(fit.scores_)\n",
    "df_cols=pd.DataFrame(x.columns)\n",
    "df_cols\n",
    "\n",
    "## getting list of features with scores corresponding to the ouput\n",
    "feature_scores=pd.concat([df_cols,df_scores],axis=1)\n",
    "feature_scores.columns=[\"Features\",\"Scores\"]\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores=feature_scores.sort_values(by=\"Scores\",ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets plot the features vs scores \n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=\"Features\",y=\"Scores\",data=feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad71fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will select the top 10 features based on scores\n",
    "feature_scores_10=feature_scores[\"Features\"][:10].to_list()\n",
    "feature_scores_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07801e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating new dataframe with the top 10 features only and the output\n",
    "\n",
    "df_10=df[['thalach',\n",
    " 'oldpeak',\n",
    " 'ca',\n",
    " 'cp',\n",
    " 'exang',\n",
    " 'age',\n",
    " 'chol',\n",
    " 'trestbps',\n",
    " 'slope',\n",
    " 'sex',\n",
    " 'target']]\n",
    "df_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_10.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e12d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for outliers\n",
    "df_10.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1deeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for outliers\n",
    "sns.boxplot(data=df_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b80165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for outliers\n",
    "sns.pairplot(df_10,hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from the boxplot we see that chol has some outliers which we will try to identify and remove the outliers\n",
    "sns.boxplot(df_10.chol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a74130",
   "metadata": {},
   "outputs": [],
   "source": [
    "## indentifying the outliers\n",
    "outliers=df[df_10.chol>450].index\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping the outliers\n",
    "after_drop=df_10.drop(index=outliers)\n",
    "after_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(after_drop.chol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaned data\n",
    "\n",
    "df_cleaned=after_drop\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ec430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81c93e",
   "metadata": {},
   "source": [
    "# 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e629037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "# applying minmax scaler transformation to my df_cleaned \n",
    "\n",
    "df_scaled=pd.DataFrame(scaler.fit_transform(df_cleaned),columns=df_cleaned.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c507a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Train data split\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "y=df_scaled[\"target\"]\n",
    "x=df_scaled.drop(\"target\",axis=1)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will check data is balanced or not\n",
    "\n",
    "df_scaled.target.value_counts()\n",
    "\n",
    "## we saw that our data is balanced so we will not do any undersampling or over sampling., we will try to plot it\n",
    "\n",
    "sns.countplot(df_scaled.target)\n",
    "plt.xlabel(\"Heart Disease\")\n",
    "plt.ylabel(\"No. of people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f484e",
   "metadata": {},
   "source": [
    "### 3.1 Model building and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aded68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "\n",
    "logit=LogisticRegression()\n",
    "logit.fit(x_train,y_train)\n",
    "y_pred=logit.predict(x_test)\n",
    "print(\"The accuracy score for Logistic is {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"The precision score for Logistic is {}\".format(precision_score(y_test,y_pred)))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)),annot=True)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Actual Value\")\n",
    "\n",
    "## Now we will check model is underfit/overfit (if any)\n",
    "accuracy_test=logit.score(x_test,y_test)\n",
    "accuracy_train=logit.score(x_train,y_train)\n",
    "\n",
    "print(\" Accuracy score in Train Data : {}\".format(accuracy_train))\n",
    "print(\" Accuracy score in Test Data : {}\".format(accuracy_test))\n",
    "\n",
    "## Model looks fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "d_tree=DecisionTreeClassifier()\n",
    "d_tree.fit(x_train,y_train)\n",
    "y_pred=d_tree.predict(x_test)\n",
    "print(\"The accuracy score for DT is {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"The precision score for DT is {}\".format(precision_score(y_test,y_pred)))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)),annot=True)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Actual Value\")\n",
    "\n",
    "## Now we will check model is underfit/overfit (if any)\n",
    "accuracy_test=d_tree.score(x_test,y_test)\n",
    "accuracy_train=d_tree.score(x_train,y_train)\n",
    "\n",
    "print(\" Accuracy score in Train Data : {}\".format(accuracy_train))\n",
    "print(\" Accuracy score in Test Data : {}\".format(accuracy_test))\n",
    "\n",
    "## Model is overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367124d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "knn.fit(x_train,y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "print(\"The accuracy score for KNN is {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"The precision score for KNN is {}\".format(precision_score(y_test,y_pred)))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)),annot=True)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Actual Value\")\n",
    "\n",
    "## Now we will check model is underfit/overfit (if any)\n",
    "accuracy_test=knn.score(x_test,y_test)\n",
    "accuracy_train=knn.score(x_train,y_train)\n",
    "\n",
    "print(\" Accuracy score in Train Data : {}\".format(accuracy_train))\n",
    "print(\" Accuracy score in Test Data : {}\".format(accuracy_test))\n",
    "\n",
    "## Model looks fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018dc4",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''From the above results we will select Logistic Regression model based on accuracy and precision score\n",
    "and also the model looks fine (not underfit/overfit). So we will do a cross validation before moving ahead to verify if the model \n",
    "will perform well to a new data set'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_score=cross_val_score(logit,x,y,cv=5)\n",
    "\n",
    "print(\"The cross validation score for Logistic Regression is: {0:.2f}%\".format(cv_score.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf3eb9",
   "metadata": {},
   "source": [
    "## Model testing with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8094793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def parameter():\n",
    "    data=pd.read_csv(\"model.csv\")\n",
    "    \n",
    "    scaler=MinMaxScaler()    \n",
    "    data_transform=pd.DataFrame(scaler.fit_transform(data),columns=data.columns)\n",
    "    \n",
    "    final_y=logit.predict(data_transform)\n",
    "    \n",
    "    for index,value in enumerate(final_y):\n",
    "        if value == 0:\n",
    "            print(\"The person {} is safe.\".format(index))\n",
    "        if value == 1:\n",
    "            print(\"The person {} is on risk.\".format(index))\n",
    "\n",
    "parameter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
